\section{Summary}
Low-grade chronic non-communicable disease (e.g., localized prostate cancer, low-grade dysplasia) patients often undergo repeated invasive \emph{tests} (biopsies, endoscopies, etc.) for confirming disease \emph{progression}. A progression is a non-terminal event, and upon progression, patients usually undergo serious treatments, e.g., surgery, radiotherapy. For detecting progression, invasive tests are typically conducted as per a one-size-fits-all (e.g., yearly) fixed schedule. Such invasive tests have two main features. First, they are indispensable because they are the benchmark/reference criteria for diagnosing progression. Second, they are burdensome for patients. Since invasive tests can only be conducted with a time gap between them, there is always a time delay in observing progression. In general, this delay is shorter when the tests are planned frequently. However, this simultaneously leads to an extra burden of biopsies on slow-progressing patients. The proportion of slow-progressing patients can be moderate to large in low-grade diseases that are the focus of our work. For the overall patient population, one-size-fits-all infrequent tests are also not a solution because they may lead to a large delay in patients who need early detection and care. Hence, our aim in this thesis was to balance better the number of tests (burden) and time delay in detecting progression (shorter is beneficial) in the whole patient population than one-size-fits-all schedules. To this end, we developed and applied statistical methods for scheduling invasive diagnostic tests (e.g., biopsies, endoscopies) in a personalized manner.

For creating personalized test schedules, our first step was to develop a statistical model. The purpose of this model was to use patients' accumulated clinical data to predict their cumulative-risk of progression over the whole follow-up period. A risk profile manifests the transition of a patient's disease state over time, from low-grade to progressed. Hence, subsequently, we used the risk profile to guide the timing of future invasive tests. That is, our second step was to obtain patient-specific test schedules using their estimated risk profiles. To this end, we optimized loss functions (e.g., squared loss) of certain clinical parameters of interest (e.g., time delay in detecting progression). The various parameters we utilized are detailed in Chapters~\ref{c2}, \ref{c3}, and \ref{c4}. In each chapter, we optimized the loss functions with respect to the estimated patient-specific cumulative-risk of progression.

The choice of loss functions and parameters lead to different types of test schedules. For example, in Chapter~\ref{c2}, we chose three standard loss functions from Bayesian decision theory. They were, namely, squared loss, absolute loss, and multilinear loss. The parameter optimized via these loss functions was the time difference between the time of the future test and the true time of progression. Squared and absolute losses resulted in tests planned at a patient's estimated mean and median time of progression, respectively. Whereas, multilinear loss planned the future test at a time point where the patient's predicted risk of progression was equal to a certain threshold. Squared and absolute loss functions aim to plan a test exactly at the true time of progression so that progression is observed without any delay. However, in doing so, they ignore the variance of the posterior predictive distribution of the time of progression of a patient. Consequently, when squared/absolute loss functions are applied repeatedly until progression is detected, they may lead to very few tests but also a large time delay in detecting progression. On the other hand, planning a test when the risk of progression is equal to a certain threshold (multilinear loss), allows patients and doctors to weigh the unnecessary tests versus time delay in detecting progression. Particularly, choosing smaller risk thresholds means a patient is willing to undergo more tests but does not want the time delay in detecting progression to be high.

In a risk threshold based approach, the key question is how to choose an appropriate threshold? Typically, thresholds are chosen based on receiver operating characteristic curve analysis or on how patients weigh the burden of an unnecessary test against a large delay in detecting progression. To further facilitate decision making for an appropriate threshold, we conducted a realistic simulation randomized controlled trial with different risk thresholds for the prostate cancer active surveillance scenario in Chapter~\ref{c3}. While the results of this simulation study are only applicable for the study cohort (PRIAS prostate cancer surveillance) to which we fitted our dataset, certain results are generalizable across all diseases. The most important of the results is that thresholds should not be chosen using measures of diagnostic accuracy, such as Youden's~J or F1~score. This is because they do not allow controlling sensitivity and specificity of a threshold.

In Chapter~\ref{c2} and Chapter~\ref{c3} we only planned one future test at a time. Later in Chapter~\ref{c4}, we extended our methodology to allow planning a full test schedule at once. We also calculated new measures of efficacy of schedules to assist patients and doctors in finding an optimal schedule. These measures were the expected number of tests and the expected time delay in detecting progression. We calculated these two in a personalized manner. Our choice of these criteria is motivated by two reasons. First, we argue that time delay in detection of progression is an easily-quantifiable surrogate for important clinical aspects such as the window of opportunity for curative treatment, risk of adverse downstream outcomes, quality-adjusted remaining lifetime, and additional complications in treating a delayed progression. Similarly, the number and timing of tests manifest financial costs of tests, risk of side-effects, and reduction in quality of life, etc. Second, both the number of tests and time delay in detecting progression are easy to understand for both patients/doctors and can facilitate \emph{shared decision making} of test schedules.

A personalized schedule is only as good as the predictive performance of the underlying statistical model. In this regard, we externally validated the model we proposed for prostate cancer active surveillance. For validation, we employed the largest six cohorts of the Movember Foundations' Global Action Plan (GAP3) database (Chapter~\ref{c5}). We calculated the time-dependent mean absolute prediction error and time-dependent area under the receiver operating characteristic curve (AUC) in each cohort. The results indicated that our model had a moderate prediction error and moderate AUC. It is important to note that our patient population had a very low risk of metastases and mortality. Also, a PRIAS based simulation study has concluded that after the first biopsy, future biopsies leading to a time delay in detecting progression up to three years may lead to very limited adverse outcomes. Thus, even with a moderate predictive performance of the model, personalized schedules based on our model can be useful for our patients. Besides, patients can review the expected time delay in detecting progression for different schedules to limit their risks. To assist them in comparing multiple schedules side by side, we also implemented biopsy schedules for real patients of the validated cohorts in a web-application (\url{https://emcbiostatistics.shinyapps.io/prias_biopsy_recommender/}). The use of personalized schedules, however, is not limited to invasive tests only. We demonstrated this by planning the NT-proBNP biomarker (a simple blood test) measurements in chronic heart failure patients using an existing personalized methodology (Chapter~\ref{c6}).